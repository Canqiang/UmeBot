# backend/app/analysis_service.py
"""æ•°æ®åˆ†ææœåŠ¡ä¼˜åŒ–ç‰ˆ"""

import asyncio
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
import pandas as pd
import numpy as np
from functools import lru_cache

from app.config import settings
from app.database import DatabaseManager
from app.fixed_causal_inference import UMeCausalInferenceEngine


class AnalysisService:
    def __init__(self):
        self.db = DatabaseManager()
        self.engine = UMeCausalInferenceEngine(settings.CLICKHOUSE_CONFIG)
        self._cache = {}
        self._cache_ttl = 300  # 5åˆ†é’Ÿç¼“å­˜

    async def initialize(self):
        """åˆå§‹åŒ–æœåŠ¡"""
        print("ğŸ“Š æ­£åœ¨åˆå§‹åŒ–åˆ†ææœåŠ¡...")
        # åˆå§‹åŒ–å› æœæ¨æ–­å¼•æ“
        await self._init_engine()

    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        self._cache.clear()

    async def _init_engine(self):
        """åˆå§‹åŒ–å› æœæ¨æ–­å¼•æ“"""
        try:
            # é¢„åŠ è½½æœ€è¿‘30å¤©çš„æ•°æ®
            end_date = datetime.now().strftime('%Y-%m-%d')
            start_date = (datetime.now() - timedelta(days=30)).strftime('%Y-%m-%d')

            loop = asyncio.get_event_loop()
            await loop.run_in_executor(
                None,
                self.engine.load_integrated_data,
                start_date,
                end_date
            )
            print("âœ… å› æœæ¨æ–­å¼•æ“åˆå§‹åŒ–å®Œæˆ")
        except Exception as e:
            print(f"âš ï¸ å› æœæ¨æ–­å¼•æ“åˆå§‹åŒ–å¤±è´¥: {e}")

    def _get_cache_key(self, method: str, params: Dict) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        import hashlib
        import json
        param_str = json.dumps(params, sort_keys=True)
        return f"{method}:{hashlib.md5(param_str.encode()).hexdigest()}"

    def _get_from_cache(self, key: str) -> Optional[Any]:
        """ä»ç¼“å­˜è·å–æ•°æ®"""
        if key in self._cache:
            data, timestamp = self._cache[key]
            if (datetime.now() - timestamp).seconds < self._cache_ttl:
                return data
        return None

    def _save_to_cache(self, key: str, data: Any):
        """ä¿å­˜åˆ°ç¼“å­˜"""
        self._cache[key] = (data, datetime.now())

    async def get_daily_report(self, start_date: str, end_date: str) -> Dict[str, Any]:
        """è·å–æ—¥æŠ¥æ•°æ®"""
        cache_key = self._get_cache_key("daily_report", {"start": start_date, "end": end_date})
        cached = self._get_from_cache(cache_key)
        if cached:
            return cached

        try:
            # ä»æ•°æ®åº“è·å–æ•°æ®
            query = f"""
            WITH first_purchase AS (
                SELECT
                    customer_id,
                    toDate(MIN(created_at_pt)) AS first_purchase_date
                FROM dw.fact_order_item_variations
                WHERE pay_status = 'COMPLETED'
                GROUP BY customer_id
            )
            SELECT
                toDate(f.created_at_pt) AS date,
                COUNT(DISTINCT f.order_id) AS order_count,
                SUM(f.item_total_amt) AS total_revenue,
                COUNT(DISTINCT f.customer_id) AS unique_customers,
                COUNT(DISTINCT f.item_name) AS item_count,
                COUNT(DISTINCT IF(fp.first_purchase_date = toDate(f.created_at_pt), f.customer_id, NULL)) AS new_users,
                AVG(f.item_total_amt) AS avg_order_value,
                SUM(f.item_discount > 0) AS discount_orders,
                SUM(f.is_loyalty) AS loyalty_orders
            FROM dw.fact_order_item_variations f
            LEFT JOIN first_purchase fp ON f.customer_id = fp.customer_id
            WHERE
                f.created_at_pt >= '{start_date}'
                AND f.created_at_pt <= '{end_date}'
                AND f.pay_status = 'COMPLETED'
            GROUP BY date
            ORDER BY date
            """

            enhanced_data = await self.db.execute_query_async(query)

            if enhanced_data.empty:
                return self._get_mock_daily_report()

            # è®¡ç®—æ—¥æŠ¥æŒ‡æ ‡
            report = {
                "date": end_date,
                "metrics": {
                    "total_revenue": float(enhanced_data['total_revenue'].sum()),
                    "total_orders": int(enhanced_data['order_count'].sum()),
                    "unique_customers": int(enhanced_data['unique_customers'].sum()),
                    "item_count": int(enhanced_data['item_count'].sum()) if 'item_count' in enhanced_data.columns else 0,
                    "new_users": int(enhanced_data['new_users'].sum()) if 'new_users' in enhanced_data.columns else 0,
                    "avg_order_value": float(enhanced_data['avg_order_value'].mean()),
                    "promotion_orders": int(enhanced_data['discount_orders'].sum()) if 'discount_orders' in enhanced_data.columns else 0,
                    "loyalty_orders": int(enhanced_data['loyalty_orders'].sum()) if 'loyalty_orders' in enhanced_data.columns else 0
                },
                "trends": self._calculate_trends(enhanced_data),
                "top_products": await self._get_top_products(start_date, end_date),
                "peak_hours": await self._get_peak_hours(start_date, end_date),
                "store_performance": await self._get_store_performance(start_date, end_date)
            }

            self._save_to_cache(cache_key, report)
            return report

        except Exception as e:
            print(f"Error getting daily report: {e}")
            import traceback
            traceback.print_exc()
            return self._get_mock_daily_report()

    def _calculate_trends(self, df: pd.DataFrame) -> Dict[str, float]:
        """è®¡ç®—è¶‹åŠ¿ï¼ˆä¿®å¤ç‰ˆï¼‰"""
        if df.empty:
            return {}

        # æŸ¥æ‰¾æ—¥æœŸåˆ—ï¼ˆæ”¯æŒå¤šç§åˆ—åï¼‰
        date_col = None
        for col in ['date', 'ds', 'order_date', 'created_date']:
            if col in df.columns:
                date_col = col
                break

        # å¦‚æœæ‰¾ä¸åˆ°æ—¥æœŸåˆ—ï¼Œå°è¯•ä½¿ç”¨ç´¢å¼•
        if date_col is None and pd.api.types.is_datetime64_any_dtype(df.index):
            df = df.reset_index()
            date_col = 'index'

        if date_col is None:
            print("Warning: No date column found for trend calculation")
            return {}

        # ç¡®ä¿æ—¥æœŸåˆ—æ˜¯datetimeç±»å‹
        df[date_col] = pd.to_datetime(df[date_col])
        df = df.sort_values(date_col)

        # åˆ†å‰²æˆå‰åä¸¤éƒ¨åˆ†
        mid_point = len(df) // 2
        first_half = df.iloc[:mid_point]
        second_half = df.iloc[mid_point:]

        trends = {}
        metrics = ['total_revenue', 'order_count', 'unique_customers']

        for metric in metrics:
            if metric in df.columns:
                first_avg = first_half[metric].mean()
                second_avg = second_half[metric].mean()
                if first_avg > 0:
                    change = ((second_avg - first_avg) / first_avg) * 100
                    trends[metric] = round(change, 2)

        return trends

    async def _get_top_products(self, start_date: str, end_date: str) -> List[Dict]:
        """è·å–çƒ­é”€äº§å“"""
        query = f"""
        SELECT
            item_name,
            COUNT(DISTINCT order_id) AS order_count,
            SUM(item_total_amt) AS revenue,
            SUM(item_qty) AS quantity
        FROM dw.fact_order_item_variations
        WHERE
            created_at_pt >= '{start_date}'
            AND created_at_pt <= '{end_date}'
            AND pay_status = 'COMPLETED'
            AND item_name IS NOT NULL
        GROUP BY item_name
        ORDER BY revenue DESC
        LIMIT 10
        """

        df = await self.db.execute_query_async(query)
        if df.empty:
            return []

        return df.to_dict('records')

    async def _get_peak_hours(self, start_date: str, end_date: str) -> List[Dict]:
        """è·å–é«˜å³°æ—¶æ®µ"""
        query = f"""
        SELECT
            toHour(created_at_pt) AS hour,
            COUNT(DISTINCT order_id) AS order_count,
            SUM(item_total_amt) AS revenue
        FROM dw.fact_order_item_variations
        WHERE
            created_at_pt >= '{start_date}'
            AND created_at_pt <= '{end_date}'
            AND pay_status = 'COMPLETED'
        GROUP BY hour
        ORDER BY revenue DESC
        """

        df = await self.db.execute_query_async(query)
        if df.empty:
            return []

        return df.head(5).to_dict('records')

    async def _get_store_performance(self, start_date: str, end_date: str) -> List[Dict]:
        """è·å–é—¨åº—è¡¨ç°"""
        query = f"""
        SELECT
            location_name,
            COUNT(DISTINCT order_id) AS order_count,
            SUM(item_total_amt) AS revenue,
            COUNT(DISTINCT customer_id) AS customers
        FROM dw.fact_order_item_variations
        WHERE
            created_at_pt >= '{start_date}'
            AND created_at_pt <= '{end_date}'
            AND pay_status = 'COMPLETED'
            AND location_name IS NOT NULL
        GROUP BY location_name
        ORDER BY revenue DESC
        LIMIT 10
        """

        df = await self.db.execute_query_async(query)
        if df.empty:
            return []

        return df.to_dict('records')

    async def get_daily_report_summary(self) -> Dict[str, Any]:
        """è·å–æ—¥æŠ¥æ‘˜è¦"""
        yesterday = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')
        today = datetime.now().strftime('%Y-%m-%d')

        report = await self.get_daily_report(yesterday, today)

        if not report:
            return None

        metrics = report.get("metrics", {})
        trends = report.get("trends", {})

        # ç”Ÿæˆæ´å¯Ÿ
        insights = []
        if trends.get("total_revenue", 0) > 10:
            insights.append(f"ğŸ“ˆ è¥æ”¶å¢é•¿æ˜¾è‘—ï¼Œç¯æ¯”ä¸Šå‡{trends['total_revenue']:.1f}%")
        elif trends.get("total_revenue", 0) < -10:
            insights.append(f"ğŸ“‰ è¥æ”¶ä¸‹é™æ˜æ˜¾ï¼Œç¯æ¯”ä¸‹é™{abs(trends['total_revenue']):.1f}%")

        if metrics.get("new_users", 0) > 0:
            insights.append(f"ğŸ‰ æ–°å¢{metrics['new_users']}ä½æ–°å®¢æˆ·")

        # ç”Ÿæˆæ‘˜è¦
        summary = {
            "date": report["date"],
            "highlights": [
                f"ğŸ’° æ€»è¥æ”¶: ${metrics.get('total_revenue', 0):,.2f}",
                f"ğŸ“¦ è®¢å•æ•°: {metrics.get('total_orders', 0):,}",
                f"ğŸ‘¥ å®¢æˆ·æ•°: {metrics.get('unique_customers', 0):,}",
                f"ğŸ›ï¸ å®¢å•ä»·: ${metrics.get('avg_order_value', 0):.2f}"
            ],
            "trends": trends,
            "insights": insights,
            "metrics": metrics
        }

        return summary

    async def get_forecast(self, days: int = 7) -> Dict[str, Any]:
        """è·å–é”€å”®é¢„æµ‹"""
        cache_key = self._get_cache_key("forecast", {"days": days})
        cached = self._get_from_cache(cache_key)
        if cached:
            return cached

        try:
            # è·å–å†å²æ•°æ®
            end_date = datetime.now().strftime('%Y-%m-%d')
            start_date = (datetime.now() - timedelta(days=60)).strftime('%Y-%m-%d')

            query = f"""
            SELECT
                toDate(created_at_pt) AS date,
                SUM(item_total_amt) AS revenue
            FROM dw.fact_order_item_variations
            WHERE
                created_at_pt >= '{start_date}'
                AND created_at_pt <= '{end_date}'
                AND pay_status = 'COMPLETED'
            GROUP BY date
            ORDER BY date
            """

            historical_data = await self.db.execute_query_async(query)

            if historical_data.empty:
                return {"error": "No historical data available"}

            # ä½¿ç”¨ç®€å•çš„ç§»åŠ¨å¹³å‡é¢„æµ‹
            historical_data['date'] = pd.to_datetime(historical_data['date'])
            historical_data = historical_data.set_index('date')

            # è®¡ç®—7å¤©ç§»åŠ¨å¹³å‡
            historical_data['ma7'] = historical_data['revenue'].rolling(window=7).mean()

            # ç”Ÿæˆé¢„æµ‹
            last_date = historical_data.index[-1]
            forecast_dates = pd.date_range(start=last_date + timedelta(days=1), periods=days)

            # ä½¿ç”¨æœ€è¿‘7å¤©çš„å¹³å‡å€¼ä½œä¸ºé¢„æµ‹åŸºå‡†
            recent_avg = historical_data['revenue'].tail(7).mean()
            recent_std = historical_data['revenue'].tail(14).std()

            # ç”Ÿæˆé¢„æµ‹æ•°æ®
            forecast_data = []
            for i, date in enumerate(forecast_dates):
                # è€ƒè™‘æ˜ŸæœŸæ•ˆåº”
                day_of_week = date.dayofweek
                week_factor = 1.0
                if day_of_week in [5, 6]:  # å‘¨æœ«
                    week_factor = 1.2
                elif day_of_week == 4:  # å‘¨äº”
                    week_factor = 1.1

                predicted = recent_avg * week_factor

                forecast_data.append({
                    "date": date.strftime("%Y-%m-%d"),
                    "actual": None,
                    "predicted": float(predicted),
                    "confidence_lower": float(predicted - 1.96 * recent_std),
                    "confidence_upper": float(predicted + 1.96 * recent_std)
                })

            # æ·»åŠ å†å²æ•°æ®
            for date, row in historical_data.tail(30).iterrows():
                forecast_data.insert(0, {
                    "date": date.strftime("%Y-%m-%d"),
                    "actual": float(row['revenue']),
                    "predicted": float(row['ma7']) if pd.notna(row['ma7']) else float(row['revenue']),
                    "confidence_lower": None,
                    "confidence_upper": None
                })

            result = {
                "forecast": {
                    "total_forecast": sum(d['predicted'] for d in forecast_data if d['actual'] is None),
                    "avg_daily_forecast": recent_avg,
                    "forecast_days": days
                },
                "chart_data": forecast_data,
                "method": "moving_average"
            }

            self._save_to_cache(cache_key, result)
            return result

        except Exception as e:
            print(f"Error getting forecast: {e}")
            import traceback
            traceback.print_exc()
            return {"error": str(e)}

    async def get_data_by_intent(self, intent: Dict[str, Any]) -> Dict[str, Any]:
        """æ ¹æ®æ„å›¾è·å–æ•°æ®"""
        intent_type = intent.get("intent_type", "general")
        time_range = intent.get("time_range", {})

        # è§£ææ—¶é—´èŒƒå›´
        start_date, end_date = self._parse_time_range(time_range)

        data = {}

        if intent_type == "daily_report":
            data = await self.get_daily_report_summary()
        elif intent_type == "forecast":
            days = intent.get("forecast_days", 7)
            data = await self.get_forecast(days)
        elif intent_type == "metrics":
            data = await self.get_metrics_data(start_date, end_date)
        elif intent_type == "analysis":
            data = await self.run_analysis(start_date, end_date)

        return data

    async def get_metrics_data(self, start_date: str, end_date: str) -> Dict[str, Any]:
        """è·å–æŒ‡æ ‡æ•°æ®"""
        query = f"""
        WITH first_purchase AS (
            SELECT
                customer_id,
                toDate(MIN(created_at_pt)) AS first_purchase_date
            FROM dw.fact_order_item_variations
            WHERE pay_status = 'COMPLETED'
            GROUP BY customer_id
        )
        SELECT
            COUNT(DISTINCT f.order_id) AS total_orders,
            SUM(f.item_total_amt) AS total_revenue,
            COUNT(DISTINCT f.customer_id) AS unique_customers,
            COUNT(DISTINCT f.item_name) AS item_count,
            COUNT(DISTINCT IF(fp.first_purchase_date >= '{start_date}', f.customer_id, NULL)) AS new_users,
            AVG(f.item_total_amt) AS avg_order_value
        FROM dw.fact_order_item_variations f
        LEFT JOIN first_purchase fp ON f.customer_id = fp.customer_id
        WHERE
            f.created_at_pt >= '{start_date}'
            AND f.created_at_pt <= '{end_date}'
            AND f.pay_status = 'COMPLETED'
        """

        df = await self.db.execute_query_async(query)

        if df.empty:
            return self._get_mock_metrics()

        return {
            "metrics": df.to_dict('records')[0],
            "display_type": "metrics_cards"
        }

    async def run_analysis(self, start_date: str, end_date: str,
                          analysis_type: str = "complete") -> Dict[str, Any]:
        """è¿è¡Œåˆ†æ"""
        try:
            loop = asyncio.get_event_loop()
            results = await loop.run_in_executor(
                None,
                self.engine.run_complete_analysis,
                start_date,
                end_date,
                True  # åŒ…å«é¢„æµ‹
            )
            return results
        except Exception as e:
            print(f"Error running analysis: {e}")
            return {"error": str(e)}

    async def get_detail_data(self, detail_type: str, params: Dict) -> Dict[str, Any]:
        """è·å–è¯¦ç»†æ•°æ®"""
        if detail_type == "top_products":
            return await self._get_top_products_detail(params)
        elif detail_type == "customer_segments":
            return await self._get_customer_segments_detail(params)
        elif detail_type == "hourly_performance":
            return await self._get_hourly_performance_detail(params)
        else:
            return {"error": "Unknown detail type"}

    async def _get_top_products_detail(self, params: Dict) -> Dict[str, Any]:
        """è·å–äº§å“è¯¦æƒ…"""
        start_date = params.get("start_date", (datetime.now() - timedelta(days=7)).strftime('%Y-%m-%d'))
        end_date = params.get("end_date", datetime.now().strftime('%Y-%m-%d'))

        products = await self._get_top_products(start_date, end_date)

        return {
            "type": "table",
            "columns": [
                {"key": "item_name", "title": "äº§å“åç§°", "type": "string"},
                {"key": "revenue", "title": "é”€å”®é¢", "type": "currency"},
                {"key": "order_count", "title": "è®¢å•æ•°", "type": "number"},
                {"key": "quantity", "title": "é”€é‡", "type": "number"}
            ],
            "rows": products
        }

    async def _get_customer_segments_detail(self, params: Dict) -> Dict[str, Any]:
        """è·å–å®¢æˆ·åˆ†ç¾¤è¯¦æƒ…"""
        query = """
        SELECT
            CASE 
                WHEN high_value_customer = 1 THEN 'é«˜ä»·å€¼å®¢æˆ·'
                WHEN loyal = 1 THEN 'å¿ è¯šå®¢æˆ·'
                WHEN regular = 1 THEN 'å¸¸è§„å®¢æˆ·'
                WHEN dormant = 1 THEN 'ä¼‘çœ å®¢æˆ·'
                ELSE 'å…¶ä»–'
            END AS segment,
            COUNT(*) AS count,
            AVG(order_final_avg_amt) AS avg_order_value,
            SUM(order_final_total_amt) AS total_revenue
        FROM ads.customer_profile
        GROUP BY segment
        ORDER BY total_revenue DESC
        """

        df = await self.db.execute_query_async(query)

        return {
            "type": "chart",
            "chart": {
                "type": "pie",
                "title": "å®¢æˆ·åˆ†ç¾¤åˆ†å¸ƒ",
                "series": df[['segment', 'count']].rename(columns={'segment': 'name', 'count': 'value'}).to_dict('records')
            }
        }

    async def _get_hourly_performance_detail(self, params: Dict) -> Dict[str, Any]:
        """è·å–å°æ—¶çº§è¡¨ç°"""
        start_date = params.get("start_date", (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d'))
        end_date = params.get("end_date", datetime.now().strftime('%Y-%m-%d'))

        peak_hours = await self._get_peak_hours(start_date, end_date)

        return {
            "type": "chart",
            "chart": {
                "type": "bar",
                "title": "å°æ—¶é”€å”®åˆ†å¸ƒ",
                "xAxis": {"type": "category", "data": [f"{h['hour']}:00" for h in peak_hours]},
                "series": [
                    {"name": "é”€å”®é¢", "data": [h['revenue'] for h in peak_hours]}
                ]
            }
        }

    def _parse_time_range(self, time_range: Dict) -> tuple:
        """è§£ææ—¶é—´èŒƒå›´"""
        if time_range.get("type") == "today":
            start = end = datetime.now().strftime('%Y-%m-%d')
        elif time_range.get("type") == "yesterday":
            date = datetime.now() - timedelta(days=1)
            start = end = date.strftime('%Y-%m-%d')
        elif time_range.get("type") == "last_n_days":
            days = time_range.get("days", 7)
            end = datetime.now().strftime('%Y-%m-%d')
            start = (datetime.now() - timedelta(days=days)).strftime('%Y-%m-%d')
        else:
            end = datetime.now().strftime('%Y-%m-%d')
            start = (datetime.now() - timedelta(days=7)).strftime('%Y-%m-%d')

        return start, end

    def _get_mock_daily_report(self) -> Dict[str, Any]:
        """è·å–æ¨¡æ‹Ÿæ—¥æŠ¥æ•°æ®"""
        return {
            "date": datetime.now().strftime('%Y-%m-%d'),
            "metrics": {
                "total_revenue": 15234.56,
                "total_orders": 142,
                "unique_customers": 89,
                "item_count": 0,
                "new_users": 0,
                "avg_order_value": 107.29,
                "promotion_orders": 23,
                "loyalty_orders": 45
            },
            "trends": {
                "total_revenue": 5.3,
                "order_count": 3.2,
                "unique_customers": 8.1
            },
            "top_products": [],
            "peak_hours": [],
            "store_performance": []
        }

    def _get_mock_metrics(self) -> Dict[str, Any]:
        """è·å–æ¨¡æ‹ŸæŒ‡æ ‡æ•°æ®"""
        return {
            "metrics": {
                "total_revenue": 52341.23,
                "total_orders": 523,
                "unique_customers": 312,
                "item_count": 156,
                "new_users": 23,
                "avg_order_value": 100.08
            },
            "display_type": "metrics_cards"
        }